#!/usr/bin/env python3
"""
ë…¼ë¬¸ ì„ë² ë”© ìƒì„± ìŠ¤í¬ë¦½íŠ¸

GROBIDë¡œ ì²˜ë¦¬ëœ ë…¼ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ Sentence Transformersë¥¼ ì‚¬ìš©í•˜ì—¬
768ì°¨ì› ì„ë² ë”©ì„ ìƒì„±í•˜ê³  PostgreSQL pgvectorì— ì €ì¥
"""

import psycopg2
import psycopg2.extras
import numpy as np
import json
from datetime import datetime
from sentence_transformers import SentenceTransformer
import torch
from typing import List, Dict, Any
import sys
import os

# ëª¨ë‹ˆí„°ë§ ëª¨ë“ˆ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from monitoring.performance_logger import measure_embedding, get_performance_logger

# PostgreSQL ì—°ê²° ì„¤ì •
DB_CONFIG = {
    'host': os.getenv('POSTGRES_HOST', 'localhost'),
    'port': int(os.getenv('POSTGRES_PORT', 5432)),
    'database': os.getenv('POSTGRES_DB', 'papers_db'),
    'user': os.getenv('POSTGRES_USER', 'postgres'),
    'password': os.getenv('POSTGRES_PASSWORD', 'postgres123')
}

class EmbeddingGenerator:
    def __init__(self, model_name='all-mpnet-base-v2'):
        """
        ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”

        Args:
            model_name: Sentence Transformers ëª¨ë¸ëª…
        """
        print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {model_name}")

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")

        # Sentence Transformers ëª¨ë¸ ë¡œë“œ
        self.model = SentenceTransformer(model_name, device=device)
        self.embedding_dim = self.model.get_sentence_embedding_dimension()

        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ - ì„ë² ë”© ì°¨ì›: {self.embedding_dim}")

    def extract_embedding_text(self, paper_data: Dict[str, Any]) -> str:
        """
        ë…¼ë¬¸ ë°ì´í„°ì—ì„œ ì„ë² ë”© ìƒì„±ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ

        Args:
            paper_data: ë…¼ë¬¸ ì •ë³´ (title, abstract_text, full_text, grobid_data)

        Returns:
            ê²°í•©ëœ í…ìŠ¤íŠ¸ (ì œëª© + ì´ˆë¡ + ì „ì²´ í…ìŠ¤íŠ¸)
        """
        text_parts = []

        # 1. ì œëª©
        if paper_data.get('title'):
            text_parts.append(f"Title: {paper_data['title']}")

        # 2. ì´ˆë¡ (ìš°ì„ ìˆœìœ„: GROBID ì¶”ì¶œ > ì›ë³¸)
        abstract = ""
        grobid_data = paper_data.get('grobid_data')
        if grobid_data:
            try:
                if isinstance(grobid_data, str):
                    grobid_data = json.loads(grobid_data)
                abstract = grobid_data.get('abstract', '')
            except (json.JSONDecodeError, TypeError):
                pass

        if not abstract and paper_data.get('abstract_text'):
            abstract = paper_data['abstract_text']

        if abstract:
            text_parts.append(f"Abstract: {abstract}")

        # 3. ì „ì²´ í…ìŠ¤íŠ¸ (GROBIDì—ì„œ ì¶”ì¶œëœ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©)
        full_text = ""
        if grobid_data and isinstance(grobid_data, dict):
            # GROBID ì„¹ì…˜ë³„ í…ìŠ¤íŠ¸ ê²°í•©
            sections = grobid_data.get('sections', [])
            if sections:
                section_texts = []
                for section in sections:
                    if section.get('content'):
                        if section.get('title'):
                            section_texts.append(f"{section['title']}: {section['content']}")
                        else:
                            section_texts.append(section['content'])
                full_text = " ".join(section_texts)
            elif grobid_data.get('full_text'):
                full_text = grobid_data['full_text']
        elif paper_data.get('full_text'):
            full_text = paper_data['full_text']

        if full_text:
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ì œí•œ (ì„ë² ë”© ëª¨ë¸ ì œí•œ ê³ ë ¤)
            max_length = 8000  # í† í° ì œí•œì„ ê³ ë ¤í•œ ë¬¸ì ìˆ˜
            if len(full_text) > max_length:
                full_text = full_text[:max_length] + "..."
            text_parts.append(f"Content: {full_text}")

        # ëª¨ë“  í…ìŠ¤íŠ¸ ê²°í•©
        combined_text = " ".join(text_parts)

        if not combined_text.strip():
            return None

        return combined_text

    def generate_embedding(self, text: str) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ì„ë² ë”© ë²¡í„° ìƒì„±

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸

        Returns:
            768ì°¨ì› ì„ë² ë”© ë²¡í„°
        """
        try:
            embedding = self.model.encode(text, convert_to_numpy=True)
            return embedding.astype(np.float32)
        except Exception as e:
            print(f"  âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def save_embedding_to_db(self, paper_id: int, embedding: np.ndarray) -> bool:
        """
        ìƒì„±ëœ ì„ë² ë”©ì„ PostgreSQLì— ì €ì¥

        Args:
            paper_id: ë…¼ë¬¸ ID
            embedding: ì„ë² ë”© ë²¡í„°

        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor() as cursor:

                    # pgvector í™•ì¥ì„ ìœ„í•œ ì„ë² ë”© í¬ë§· ë³€í™˜
                    embedding_str = '[' + ','.join(map(str, embedding)) + ']'

                    cursor.execute("""
                        UPDATE papers
                        SET embedding = %s::vector,
                            embedding_model = 'all-mpnet-base-v2',
                            embedding_generated_at = NOW(),
                            updated_at = NOW()
                        WHERE id = %s
                    """, (embedding_str, paper_id))

                    return True

        except Exception as e:
            print(f"  âŒ ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def process_papers_for_embeddings(self):
        """
        ëª¨ë“  í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë…¼ë¬¸ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±
        """
        try:
            # í…ìŠ¤íŠ¸ê°€ ìˆê³  ì„ë² ë”©ì´ ì—†ëŠ” ë…¼ë¬¸ë“¤ ì¡°íšŒ
            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:

                    cursor.execute("""
                        SELECT id, title, abstract_text, full_text, grobid_data
                        FROM papers
                        WHERE (full_text IS NOT NULL OR grobid_data IS NOT NULL)
                        AND embedding IS NULL
                        ORDER BY id
                    """)

                    papers = cursor.fetchall()
                    print(f"ğŸš€ ì„ë² ë”© ìƒì„± ëŒ€ìƒ: {len(papers)}ê°œ ë…¼ë¬¸")

            if not papers:
                print("âœ… ëª¨ë“  ë…¼ë¬¸ì— ì´ë¯¸ ì„ë² ë”©ì´ ìˆìŠµë‹ˆë‹¤.")
                return

            success_count = 0
            failed_count = 0

            for i, paper in enumerate(papers, 1):
                print(f"\nğŸ“„ ë…¼ë¬¸ {i}/{len(papers)} ì²˜ë¦¬ ì¤‘...")
                print(f"  ID: {paper['id']}")
                print(f"  ì œëª©: {paper['title'][:60]}...")

                try:
                    # 1. ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    embedding_text = self.extract_embedding_text(dict(paper))

                    if not embedding_text:
                        print(f"  âš ï¸ ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŒ")
                        failed_count += 1
                        continue

                    print(f"  ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(embedding_text)} ë¬¸ì")

                    # 2. ì„ë² ë”© ìƒì„±
                    embedding = self.generate_embedding(embedding_text)

                    if embedding is None:
                        failed_count += 1
                        continue

                    print(f"  ğŸ¤– ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embedding.shape}")

                    # 3. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                    success = self.save_embedding_to_db(paper['id'], embedding)

                    if success:
                        success_count += 1
                        print(f"  âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ")
                    else:
                        failed_count += 1

                except Exception as e:
                    print(f"  âŒ ë…¼ë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    failed_count += 1
                    continue

            print(f"\nğŸ‰ ì„ë² ë”© ìƒì„± ì™„ë£Œ:")
            print(f"  ì„±ê³µ: {success_count}ê°œ")
            print(f"  ì‹¤íŒ¨: {failed_count}ê°œ")

            # ìµœì¢… í†µê³„ ì¶œë ¥
            self.print_embedding_status()

        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")

    def print_embedding_status(self):
        """ì„ë² ë”© ìƒíƒœ í†µê³„ ì¶œë ¥"""
        try:
            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT
                            COUNT(*) as total_papers,
                            COUNT(CASE WHEN embedding IS NOT NULL THEN 1 END) as with_embedding,
                            COUNT(CASE WHEN full_text IS NOT NULL OR grobid_data IS NOT NULL THEN 1 END) as with_text,
                            COUNT(CASE WHEN grobid_status = 'completed' THEN 1 END) as grobid_completed
                        FROM papers
                    """)

                    stats = cursor.fetchone()

                    print(f"\nğŸ“Š ì„ë² ë”© í˜„í™©:")
                    print(f"  ì „ì²´ ë…¼ë¬¸: {stats[0]}ê°œ")
                    print(f"  í…ìŠ¤íŠ¸ ìˆìŒ: {stats[2]}ê°œ")
                    print(f"  GROBID ì™„ë£Œ: {stats[3]}ê°œ")
                    print(f"  ì„ë² ë”© ìˆìŒ: {stats[1]}ê°œ")

                    # ì„ë² ë”© ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
                    if stats[2] > 0:
                        coverage = (stats[1] / stats[2]) * 100
                        print(f"  ì„ë² ë”© ì»¤ë²„ë¦¬ì§€: {coverage:.1f}%")

        except Exception as e:
            print(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    def test_similarity_search(self, query: str, top_k: int = 5):
        """
        ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ìƒìœ„ Kê°œ ê²°ê³¼
        """
        try:
            print(f"\nğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{query}'")

            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.generate_embedding(query)
            if query_embedding is None:
                print("âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                return

            query_embedding_str = '[' + ','.join(map(str, query_embedding)) + ']'

            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:

                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰
                    cursor.execute("""
                        SELECT
                            id, title,
                            1 - (embedding <=> %s::vector) as similarity
                        FROM papers
                        WHERE embedding IS NOT NULL
                        ORDER BY embedding <=> %s::vector
                        LIMIT %s
                    """, (query_embedding_str, query_embedding_str, top_k))

                    results = cursor.fetchall()

                    print(f"ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ {len(results)}ê°œ):")
                    for i, result in enumerate(results, 1):
                        print(f"  {i}. [ID:{result['id']}] {result['title'][:80]}...")
                        print(f"     ìœ ì‚¬ë„: {result['similarity']:.4f}")

        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    generator = EmbeddingGenerator()

    # ì„ë² ë”© ìƒì„±
    generator.process_papers_for_embeddings()

    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    generator.test_similarity_search("deep learning neural networks", top_k=3)
    generator.test_similarity_search("computer vision image classification", top_k=3)

if __name__ == "__main__":
    main()