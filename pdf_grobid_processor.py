#!/usr/bin/env python3
"""
PDF ë‹¤ìš´ë¡œë“œ ë° GROBID ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

ì €ì¥ëœ ë…¼ë¬¸ë“¤ì˜ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  GROBIDë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œ ì €ì¥
"""

import requests
import psycopg2
import psycopg2.extras
import json
import tempfile
import os
from datetime import datetime
import time
from pathlib import Path

# PostgreSQL ì—°ê²° ì„¤ì •
DB_CONFIG = {
    'host': os.getenv('POSTGRES_HOST', 'localhost'),
    'port': int(os.getenv('POSTGRES_PORT', 5432)),
    'database': os.getenv('POSTGRES_DB', 'papers_db'),
    'user': os.getenv('POSTGRES_USER', 'postgres'),
    'password': os.getenv('POSTGRES_PASSWORD', 'postgres123')
}

# GROBID ì„œë¹„ìŠ¤ ì„¤ì •
GROBID_URL = os.getenv('GROBID_URL', "http://localhost:8070")

class PDFGrobidProcessor:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/pdf,application/octet-stream,*/*;q=0.9',
            'Referer': 'https://www.google.com/',
            'Accept-Language': 'en-US,en;q=0.9'
        })

    def download_pdf(self, pdf_url, max_size_mb=50):
        """PDF ë‹¤ìš´ë¡œë“œ"""
        try:
            print(f"  ğŸ“¥ PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {pdf_url}")

            # HEAD ìš”ì²­ìœ¼ë¡œ íŒŒì¼ í¬ê¸° í™•ì¸
            head_response = self.session.head(pdf_url, timeout=10)
            content_length = head_response.headers.get('content-length')

            if content_length:
                size_mb = int(content_length) / (1024 * 1024)
                if size_mb > max_size_mb:
                    print(f"  âš ï¸ PDF íŒŒì¼ì´ ë„ˆë¬´ í¼: {size_mb:.1f}MB (ì œí•œ: {max_size_mb}MB)")
                    return None

            # PDF ë‹¤ìš´ë¡œë“œ
            response = self.session.get(pdf_url, timeout=30)
            response.raise_for_status()

            # Content-Type í™•ì¸ (ë„ˆë¬´ ì—„ê²©í•˜ì§€ ì•Šê²Œ)
            content_type = response.headers.get('content-type', '')
            if 'pdf' not in content_type.lower() and not response.content.startswith(b'%PDF'):
                print(f"  âš ï¸ PDFê°€ ì•„ë‹Œ íŒŒì¼: {content_type}")
                return None

            print(f"  âœ… PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(response.content)} bytes")
            return response.content

        except requests.exceptions.Timeout:
            print(f"  âŒ PDF ë‹¤ìš´ë¡œë“œ íƒ€ì„ì•„ì›ƒ: {pdf_url}")
        except requests.exceptions.RequestException as e:
            print(f"  âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        except Exception as e:
            print(f"  âŒ PDF ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")

        return None

    def process_with_grobid(self, pdf_content):
        """GROBIDë¡œ PDF ì²˜ë¦¬í•˜ì—¬ JSON êµ¬ì¡°í™” ë°ì´í„° ì¶”ì¶œ"""
        try:
            print(f"  ğŸ” GROBID ì²˜ë¦¬ ì¤‘...")

            # GROBID fulltext ì—”ë“œí¬ì¸íŠ¸ë¡œ ìš”ì²­
            files = {'input': ('paper.pdf', pdf_content, 'application/pdf')}
            data = {
                'consolidateHeader': '1',
                'consolidateCitations': '1',
                'includeRawCitations': '1',
                'includeRawAffiliations': '1'
            }

            response = self.session.post(
                f"{GROBID_URL}/api/processFulltextDocument",
                files=files,
                data=data,
                timeout=60
            )

            if response.status_code != 200:
                print(f"  âŒ GROBID ì²˜ë¦¬ ì‹¤íŒ¨: HTTP {response.status_code}")
                return None

            # XML ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
            xml_content = response.text
            structured_data = self.parse_grobid_xml(xml_content)

            print(f"  âœ… GROBID ì²˜ë¦¬ ì™„ë£Œ")
            return structured_data

        except requests.exceptions.Timeout:
            print(f"  âŒ GROBID ì²˜ë¦¬ íƒ€ì„ì•„ì›ƒ")
        except Exception as e:
            print(f"  âŒ GROBID ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

        return None

    def parse_grobid_xml(self, xml_content):
        """GROBID XMLì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ JSON ë°ì´í„° ìƒì„±"""
        try:
            from xml.etree import ElementTree as ET

            # XML íŒŒì‹±
            root = ET.fromstring(xml_content)

            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì²˜ë¦¬
            namespaces = {
                'tei': 'http://www.tei-c.org/ns/1.0'
            }

            structured_data = {
                'title': '',
                'abstract': '',
                'full_text': '',
                'authors': [],
                'sections': [],
                'references': [],
                'keywords': [],
                'metadata': {},
                'processing_info': {
                    'processed_at': datetime.now().isoformat(),
                    'grobid_version': 'grobid-0.7.3',
                    'source': 'grobid_fulltext'
                }
            }

            # ì œëª© ì¶”ì¶œ
            title_elem = root.find('.//tei:titleStmt/tei:title', namespaces)
            if title_elem is not None and title_elem.text:
                structured_data['title'] = title_elem.text.strip()

            # ì´ˆë¡ ì¶”ì¶œ
            abstract_elem = root.find('.//tei:abstract', namespaces)
            if abstract_elem is not None:
                abstract_text = ''.join(abstract_elem.itertext()).strip()
                structured_data['abstract'] = abstract_text

            # í‚¤ì›Œë“œ ì¶”ì¶œ (ì €ìê°€ ì œê³µí•œ í‚¤ì›Œë“œ)
            keywords = []

            # ë°©ë²• 1: keywords íƒœê·¸ì—ì„œ ì¶”ì¶œ
            for keyword_elem in root.findall('.//tei:keywords//tei:term', namespaces):
                if keyword_elem.text:
                    keyword_text = keyword_elem.text.strip()
                    if keyword_text:
                        keywords.append(keyword_text)

            # ë°©ë²• 2: textClassì˜ í‚¤ì›Œë“œì—ì„œë„ ì¶”ì¶œ
            for keyword_elem in root.findall('.//tei:textClass//tei:keywords//tei:term', namespaces):
                if keyword_elem.text:
                    keyword_text = keyword_elem.text.strip()
                    if keyword_text and keyword_text not in keywords:
                        keywords.append(keyword_text)

            # ë°©ë²• 3: ë‹¤ë¥¸ í˜•íƒœì˜ í‚¤ì›Œë“œ íƒœê·¸ í™•ì¸
            for keyword_elem in root.findall('.//tei:keywords/tei:list/tei:item', namespaces):
                if keyword_elem.text:
                    keyword_text = keyword_elem.text.strip()
                    if keyword_text and keyword_text not in keywords:
                        keywords.append(keyword_text)

            structured_data['keywords'] = keywords

            # ì €ì ì •ë³´ ì¶”ì¶œ
            for author_elem in root.findall('.//tei:author', namespaces):
                author_info = {}

                # ì´ë¦„ ì¶”ì¶œ
                forename = author_elem.find('.//tei:forename', namespaces)
                surname = author_elem.find('.//tei:surname', namespaces)

                if forename is not None and surname is not None:
                    author_info['name'] = f"{forename.text} {surname.text}".strip()
                elif surname is not None:
                    author_info['name'] = surname.text.strip()

                # ì†Œì† ì¶”ì¶œ
                affiliation = author_elem.find('.//tei:affiliation', namespaces)
                if affiliation is not None:
                    org_name = affiliation.find('.//tei:orgName', namespaces)
                    if org_name is not None:
                        author_info['affiliation'] = org_name.text.strip()

                if author_info.get('name'):
                    structured_data['authors'].append(author_info)

            # ì„¹ì…˜ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text_parts = []
            for div_elem in root.findall('.//tei:body//tei:div', namespaces):
                section_data = {}

                # ì„¹ì…˜ ì œëª©
                head_elem = div_elem.find('tei:head', namespaces)
                if head_elem is not None and head_elem.text:
                    section_data['title'] = head_elem.text.strip()

                # ì„¹ì…˜ ë‚´ìš©
                section_text = []
                for p_elem in div_elem.findall('.//tei:p', namespaces):
                    p_text = ''.join(p_elem.itertext()).strip()
                    if p_text:
                        section_text.append(p_text)

                if section_text:
                    section_content = '\n\n'.join(section_text)
                    section_data['content'] = section_content
                    structured_data['sections'].append(section_data)
                    full_text_parts.append(section_content)

            # ì „ì²´ í…ìŠ¤íŠ¸ ì¡°í•©
            structured_data['full_text'] = '\n\n'.join(full_text_parts)

            # ì°¸ê³ ë¬¸í—Œ ì¶”ì¶œ
            for ref_elem in root.findall('.//tei:listBibl/tei:biblStruct', namespaces):
                ref_data = {}

                # ì œëª©
                title = ref_elem.find('.//tei:title[@level="a"]', namespaces)
                if title is not None and title.text:
                    ref_data['title'] = title.text.strip()

                # ì €ìë“¤
                ref_authors = []
                for ref_author in ref_elem.findall('.//tei:author', namespaces):
                    forename = ref_author.find('tei:forename', namespaces)
                    surname = ref_author.find('tei:surname', namespaces)

                    if surname is not None:
                        if forename is not None:
                            ref_authors.append(f"{forename.text} {surname.text}".strip())
                        else:
                            ref_authors.append(surname.text.strip())

                if ref_authors:
                    ref_data['authors'] = ref_authors

                # ì¶œíŒ ì •ë³´
                journal = ref_elem.find('.//tei:title[@level="j"]', namespaces)
                if journal is not None and journal.text:
                    ref_data['journal'] = journal.text.strip()

                date_elem = ref_elem.find('.//tei:date', namespaces)
                if date_elem is not None and date_elem.get('when'):
                    ref_data['year'] = date_elem.get('when')

                if ref_data:
                    structured_data['references'].append(ref_data)

            # ë©”íƒ€ë°ì´í„°
            structured_data['metadata'] = {
                'sections_count': len(structured_data['sections']),
                'references_count': len(structured_data['references']),
                'authors_count': len(structured_data['authors']),
                'keywords_count': len(structured_data['keywords']),
                'text_length': len(structured_data['full_text']),
                'has_abstract': bool(structured_data['abstract'])
            }

            return structured_data

        except ET.ParseError as e:
            print(f"  âŒ XML íŒŒì‹± ì‹¤íŒ¨: {e}")
            # XML íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ í…ìŠ¤íŠ¸ë¡œ ì €ì¥
            return {
                'full_text': xml_content,
                'processing_info': {
                    'processed_at': datetime.now().isoformat(),
                    'source': 'grobid_raw_xml',
                    'parse_error': str(e)
                }
            }
        except Exception as e:
            print(f"  âŒ GROBID XML íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def update_paper_grobid_data(self, paper_id, grobid_data, full_text):
        """ë…¼ë¬¸ì˜ GROBID ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë°ì´íŠ¸"""
        try:
            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor() as cursor:
                    # í‚¤ì›Œë“œ ì¶”ì¶œ
                    keywords = grobid_data.get('keywords', [])
                    keywords_json = json.dumps(keywords) if keywords else None

                    cursor.execute("""
                        UPDATE papers
                        SET full_text = %s,
                            grobid_data = %s,
                            keywords = %s,
                            grobid_status = 'completed',
                            updated_at = NOW()
                        WHERE id = %s
                    """, (full_text, json.dumps(grobid_data), keywords_json, paper_id))

                    print(f"  ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    print(f"      í‚¤ì›Œë“œ {len(keywords)}ê°œ ì €ì¥: {keywords}")
                    return True

        except Exception as e:
            print(f"  âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False

    def mark_paper_failed(self, paper_id, error_msg):
        """ë…¼ë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨ ìƒíƒœë¡œ ë§ˆí‚¹"""
        try:
            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        UPDATE papers
                        SET grobid_status = 'failed',
                            grobid_data = %s,
                            updated_at = NOW()
                        WHERE id = %s
                    """, (json.dumps({'error': error_msg, 'failed_at': datetime.now().isoformat()}), paper_id))

        except Exception as e:
            print(f"  âŒ ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def process_all_papers(self):
        """ì €ì¥ëœ ëª¨ë“  ë…¼ë¬¸ ì²˜ë¦¬"""
        try:
            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:

                    # ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë…¼ë¬¸ë“¤ ì¡°íšŒ
                    cursor.execute("""
                        SELECT id, openalex_paper_id, title, pdf_url
                        FROM papers
                        WHERE pdf_url IS NOT NULL
                        AND (grobid_status IS NULL OR grobid_status = 'pending')
                        ORDER BY id
                    """)

                    papers = cursor.fetchall()
                    print(f"ğŸš€ ì²˜ë¦¬í•  ë…¼ë¬¸: {len(papers)}ê°œ")

            processed_count = 0
            failed_count = 0

            for i, paper in enumerate(papers, 1):
                print(f"\nğŸ“„ ë…¼ë¬¸ {i}/{len(papers)} ì²˜ë¦¬ ì¤‘...")
                print(f"  ì œëª©: {paper['title'][:60]}...")
                print(f"  PDF: {paper['pdf_url']}")

                try:
                    # 1. PDF ë‹¤ìš´ë¡œë“œ
                    pdf_content = self.download_pdf(paper['pdf_url'])
                    if not pdf_content:
                        self.mark_paper_failed(paper['id'], "PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                        failed_count += 1
                        continue

                    # 2. GROBID ì²˜ë¦¬
                    grobid_data = self.process_with_grobid(pdf_content)
                    if not grobid_data:
                        self.mark_paper_failed(paper['id'], "GROBID ì²˜ë¦¬ ì‹¤íŒ¨")
                        failed_count += 1
                        continue

                    # 3. ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
                    full_text = grobid_data.get('full_text', '')
                    success = self.update_paper_grobid_data(paper['id'], grobid_data, full_text)

                    if success:
                        processed_count += 1
                        print(f"  âœ… ì²˜ë¦¬ ì™„ë£Œ (ì „ì²´ í…ìŠ¤íŠ¸: {len(full_text)} ë¬¸ì)")
                    else:
                        failed_count += 1

                    # API ì œí•œ ê³ ë ¤í•˜ì—¬ ì ì‹œ ëŒ€ê¸°
                    time.sleep(1)

                except Exception as e:
                    print(f"  âŒ ë…¼ë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    self.mark_paper_failed(paper['id'], str(e))
                    failed_count += 1

            print(f"\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ:")
            print(f"  ì„±ê³µ: {processed_count}ê°œ")
            print(f"  ì‹¤íŒ¨: {failed_count}ê°œ")

            # ìµœì¢… ìƒíƒœ ì¶œë ¥
            self.print_processing_status()

        except Exception as e:
            print(f"âŒ ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    def print_processing_status(self):
        """ì²˜ë¦¬ ìƒíƒœ í†µê³„ ì¶œë ¥"""
        try:
            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT
                            COUNT(*) as total,
                            COUNT(CASE WHEN grobid_status = 'completed' THEN 1 END) as completed,
                            COUNT(CASE WHEN grobid_status = 'failed' THEN 1 END) as failed,
                            COUNT(CASE WHEN grobid_status IS NULL OR grobid_status = 'pending' THEN 1 END) as pending,
                            COUNT(CASE WHEN full_text IS NOT NULL AND length(full_text) > 0 THEN 1 END) as with_text
                        FROM papers
                    """)

                    stats = cursor.fetchone()

                    print(f"\nğŸ“Š ë…¼ë¬¸ ì²˜ë¦¬ í˜„í™©:")
                    print(f"  ì „ì²´ ë…¼ë¬¸: {stats[0]}ê°œ")
                    print(f"  ì²˜ë¦¬ ì™„ë£Œ: {stats[1]}ê°œ")
                    print(f"  ì²˜ë¦¬ ì‹¤íŒ¨: {stats[2]}ê°œ")
                    print(f"  ì²˜ë¦¬ ëŒ€ê¸°: {stats[3]}ê°œ")
                    print(f"  í…ìŠ¤íŠ¸ ìˆìŒ: {stats[4]}ê°œ")

        except Exception as e:
            print(f"âŒ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")

def main():
    processor = PDFGrobidProcessor()
    processor.process_all_papers()

if __name__ == "__main__":
    main()