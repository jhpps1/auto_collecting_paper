#!/usr/bin/env python3
"""
ë…¼ë¬¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë³‘ëª© ë¶„ì„ ë° ë²¤ì¹˜ë§ˆí¬

ê° ë‹¨ê³„ë³„ ì²˜ë¦¬ ì†ë„ ì¸¡ì •:
1. OpenAlex ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
2. PDF ë‹¤ìš´ë¡œë“œ + GROBID ì²˜ë¦¬
3. ì„ë² ë”© ìƒì„±
4. ìœ ì‚¬ë„ ê³„ì‚° + HBase ì €ì¥
"""

import time
import requests
import psycopg2
import psycopg2.extras
import numpy as np
from datetime import datetime
import subprocess
import json
import base64
from typing import Dict, List, Tuple
import statistics

# PostgreSQL ì—°ê²° ì„¤ì •
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'papers_db',
    'user': 'postgres',
    'password': 'postgres123'
}

class PipelineBenchmark:
    def __init__(self):
        self.results = {
            'openalex_metadata': [],
            'pdf_grobid': [],
            'embedding': [],
            'similarity_hbase': []
        }

    def benchmark_openalex_metadata(self, count=5):
        """OpenAlex ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ë²¤ì¹˜ë§ˆí¬"""
        print(f"ğŸ” 1. OpenAlex ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ë²¤ì¹˜ë§ˆí¬ ({count}ê°œ ë…¼ë¬¸)")

        base_url = "https://api.openalex.org/works"
        headers = {'User-Agent': 'Mozilla/5.0 (RSP-Paper-System/1.0; mailto:test@example.com)'}

        times = []

        for i in range(count):
            start_time = time.time()

            try:
                params = {
                    'search': f'machine learning page:{i+1}',
                    'filter': 'concepts.id:C41008148,type_crossref:journal-article,open_access.is_oa:true,has_doi:true',
                    'per-page': 1,
                    'sort': 'cited_by_count:desc'
                }

                response = requests.get(base_url, params=params, headers=headers, timeout=10)

                if response.status_code == 200:
                    data = response.json()
                    paper = data['results'][0] if data['results'] else None

                    if paper:
                        elapsed = time.time() - start_time
                        times.append(elapsed)
                        print(f"   ë…¼ë¬¸ {i+1}: {elapsed:.2f}ì´ˆ - {paper['title'][:50]}...")
                    else:
                        print(f"   ë…¼ë¬¸ {i+1}: ë°ì´í„° ì—†ìŒ")
                else:
                    print(f"   ë…¼ë¬¸ {i+1}: API ì˜¤ë¥˜ {response.status_code}")

            except Exception as e:
                print(f"   ë…¼ë¬¸ {i+1}: ì˜¤ë¥˜ - {e}")

            # API ì œí•œ ë°©ì§€
            time.sleep(0.1)

        if times:
            avg_time = statistics.mean(times)
            self.results['openalex_metadata'] = times
            print(f"   ğŸ“Š í‰ê·  ì‹œê°„: {avg_time:.2f}ì´ˆ/ë…¼ë¬¸")
            print(f"   ğŸ“Š ì²˜ë¦¬ ì†ë„: {3600/avg_time:.0f}ë…¼ë¬¸/ì‹œê°„")

        return times

    def benchmark_pdf_grobid(self, sample_size=3):
        """PDF + GROBID ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸ“„ 2. PDF ë‹¤ìš´ë¡œë“œ + GROBID ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬ ({sample_size}ê°œ ë…¼ë¬¸)")

        # PDF URLì´ ìˆëŠ” ë…¼ë¬¸ ìƒ˜í”Œ ì¡°íšŒ
        try:
            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
                    cursor.execute("""
                        SELECT id, title, pdf_url
                        FROM papers
                        WHERE pdf_url IS NOT NULL
                        AND grobid_status IS NULL
                        LIMIT %s
                    """, (sample_size,))
                    papers = cursor.fetchall()
        except Exception as e:
            print(f"   âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

        if not papers:
            print("   âš ï¸ ì²˜ë¦¬í•  PDFê°€ ì—†ìŠµë‹ˆë‹¤")
            return []

        times = []
        grobid_url = "http://localhost:8070/api/processFulltextDocument"

        for paper in papers:
            start_time = time.time()

            try:
                print(f"   ì²˜ë¦¬ ì¤‘: {paper['title'][:50]}...")

                # PDF ë‹¤ìš´ë¡œë“œ
                pdf_start = time.time()
                pdf_response = requests.get(paper['pdf_url'], timeout=30)
                pdf_download_time = time.time() - pdf_start

                if pdf_response.status_code == 200:
                    # GROBID ì²˜ë¦¬
                    grobid_start = time.time()
                    files = {'input': ('paper.pdf', pdf_response.content, 'application/pdf')}
                    grobid_response = requests.post(grobid_url, files=files, timeout=60)
                    grobid_process_time = time.time() - grobid_start

                    if grobid_response.status_code == 200:
                        total_time = time.time() - start_time
                        times.append(total_time)
                        print(f"     âœ… ì™„ë£Œ: {total_time:.2f}ì´ˆ (PDF: {pdf_download_time:.2f}ì´ˆ, GROBID: {grobid_process_time:.2f}ì´ˆ)")
                    else:
                        print(f"     âŒ GROBID ì‹¤íŒ¨: {grobid_response.status_code}")
                else:
                    print(f"     âŒ PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {pdf_response.status_code}")

            except Exception as e:
                print(f"     âŒ ì˜¤ë¥˜: {e}")

        if times:
            avg_time = statistics.mean(times)
            self.results['pdf_grobid'] = times
            print(f"   ğŸ“Š í‰ê·  ì‹œê°„: {avg_time:.2f}ì´ˆ/ë…¼ë¬¸")
            print(f"   ğŸ“Š ì²˜ë¦¬ ì†ë„: {3600/avg_time:.0f}ë…¼ë¬¸/ì‹œê°„")

        return times

    def benchmark_embedding_generation(self, sample_size=5):
        """ì„ë² ë”© ìƒì„± ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸ”„ 3. ì„ë² ë”© ìƒì„± ë²¤ì¹˜ë§ˆí¬ ({sample_size}ê°œ ë…¼ë¬¸)")

        # ì„ë² ë”©ì´ ì—†ëŠ” ë…¼ë¬¸ ìƒ˜í”Œ ì¡°íšŒ
        try:
            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
                    cursor.execute("""
                        SELECT id, title, abstract, full_text
                        FROM papers
                        WHERE embedding IS NULL
                        AND (abstract IS NOT NULL OR full_text IS NOT NULL)
                        LIMIT %s
                    """, (sample_size,))
                    papers = cursor.fetchall()
        except Exception as e:
            print(f"   âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

        if not papers:
            print("   âš ï¸ ì²˜ë¦¬í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return []

        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œê°„ ì¸¡ì •
        print("   ğŸ”§ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        model_start = time.time()

        try:
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
            model_load_time = time.time() - model_start
            print(f"   âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_load_time:.2f}ì´ˆ")
        except Exception as e:
            print(f"   âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return []

        times = []

        for paper in papers:
            start_time = time.time()

            try:
                # í…ìŠ¤íŠ¸ ì¤€ë¹„
                text_parts = []
                if paper['title']:
                    text_parts.append(paper['title'])
                if paper['abstract']:
                    text_parts.append(paper['abstract'])
                if paper['full_text']:
                    text_parts.append(paper['full_text'][:5000])  # ì²˜ìŒ 5000ìë§Œ

                combined_text = ' '.join(text_parts)

                if combined_text.strip():
                    # ì„ë² ë”© ìƒì„±
                    embedding = model.encode(combined_text)

                    # DB ì €ì¥
                    embedding_list = embedding.tolist()

                    with psycopg2.connect(**DB_CONFIG) as conn:
                        with conn.cursor() as cursor:
                            cursor.execute("""
                                UPDATE papers
                                SET embedding = %s,
                                    embedding_model = %s,
                                    embedding_generated_at = %s
                                WHERE id = %s
                            """, (str(embedding_list), 'all-mpnet-base-v2', datetime.now(), paper['id']))
                            conn.commit()

                    elapsed = time.time() - start_time
                    times.append(elapsed)
                    print(f"   ë…¼ë¬¸ {paper['id']}: {elapsed:.2f}ì´ˆ - {paper['title'][:50]}...")

            except Exception as e:
                print(f"   ë…¼ë¬¸ {paper['id']}: ì˜¤ë¥˜ - {e}")

        if times:
            avg_time = statistics.mean(times)
            self.results['embedding'] = times
            print(f"   ğŸ“Š í‰ê·  ì‹œê°„: {avg_time:.2f}ì´ˆ/ë…¼ë¬¸ (ëª¨ë¸ ë¡œë”© ì œì™¸)")
            print(f"   ğŸ“Š ì²˜ë¦¬ ì†ë„: {3600/avg_time:.0f}ë…¼ë¬¸/ì‹œê°„")
            print(f"   ğŸ“Š ëª¨ë¸ ë¡œë”©: {model_load_time:.2f}ì´ˆ (1íšŒ)")

        return times

    def benchmark_similarity_calculation(self, sample_size=10):
        """ìœ ì‚¬ë„ ê³„ì‚° ë²¤ì¹˜ë§ˆí¬"""
        print(f"\nğŸ”¢ 4. ìœ ì‚¬ë„ ê³„ì‚° ë²¤ì¹˜ë§ˆí¬ ({sample_size}ê°œ ë…¼ë¬¸)")

        # ì„ë² ë”©ì´ ìˆëŠ” ë…¼ë¬¸ë“¤ ì¡°íšŒ
        try:
            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
                    cursor.execute("""
                        SELECT id, title, embedding
                        FROM papers
                        WHERE embedding IS NOT NULL
                        LIMIT %s
                    """, (sample_size,))
                    papers = cursor.fetchall()
        except Exception as e:
            print(f"   âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

        if len(papers) < 2:
            print("   âš ï¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ë…¼ë¬¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 2ê°œ í•„ìš”)")
            return []

        print(f"   ğŸ“Š ì´ ë¹„êµ íšŸìˆ˜: {len(papers) * (len(papers) - 1) // 2}ê°œ")

        # ì„ë² ë”© íŒŒì‹±
        embeddings_data = []
        for paper in papers:
            try:
                embedding_str = paper['embedding']
                if embedding_str.startswith('[') and embedding_str.endswith(']'):
                    embedding_str = embedding_str[1:-1]

                embedding_values = [float(x.strip()) for x in embedding_str.split(',')]
                embedding_array = np.array(embedding_values, dtype=np.float32)

                embeddings_data.append({
                    'paper_id': paper['id'],
                    'title': paper['title'],
                    'embedding': embedding_array
                })
            except Exception as e:
                print(f"   âš ï¸ ë…¼ë¬¸ {paper['id']} ì„ë² ë”© íŒŒì‹± ì‹¤íŒ¨: {e}")

        if len(embeddings_data) < 2:
            print("   âŒ ìœ íš¨í•œ ì„ë² ë”©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
            return []

        # ìœ ì‚¬ë„ ê³„ì‚° ì‹œê°„ ì¸¡ì •
        start_time = time.time()

        similarities = []
        comparison_count = 0

        for i, paper_a in enumerate(embeddings_data):
            paper_similarities = []

            for j, paper_b in enumerate(embeddings_data):
                if i != j:
                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    vec1 = paper_a['embedding']
                    vec2 = paper_b['embedding']

                    dot_product = np.dot(vec1, vec2)
                    norm_a = np.linalg.norm(vec1)
                    norm_b = np.linalg.norm(vec2)

                    if norm_a != 0 and norm_b != 0:
                        similarity = dot_product / (norm_a * norm_b)
                        paper_similarities.append({
                            'target_paper_id': paper_b['paper_id'],
                            'similarity': float(similarity)
                        })
                        comparison_count += 1

            # Top-K ì„ íƒ (ì—¬ê¸°ì„œëŠ” ëª¨ë“  ìœ ì‚¬ë„)
            paper_similarities.sort(key=lambda x: x['similarity'], reverse=True)
            similarities.append({
                'source_paper_id': paper_a['paper_id'],
                'similarities': paper_similarities
            })

        calculation_time = time.time() - start_time

        print(f"   âœ… ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ: {calculation_time:.2f}ì´ˆ")
        print(f"   ğŸ“Š ë¹„êµ íšŸìˆ˜: {comparison_count}ê°œ")
        print(f"   ğŸ“Š ê³„ì‚° ì†ë„: {comparison_count/calculation_time:.0f}íšŒ/ì´ˆ")

        # HBase ì €ì¥ ì‹œê°„ ì¸¡ì • (ìƒ˜í”Œë§Œ)
        hbase_start = time.time()

        try:
            hbase_url = "http://localhost:8080"
            sample_paper = similarities[0]

            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥
            test_data = {
                'meta:source_title': f"benchmark_test_{int(time.time())}",
                'meta:total_similarities': str(len(sample_paper['similarities'])),
                'similar:rank_01_paper_id': str(sample_paper['similarities'][0]['target_paper_id']),
                'similar:rank_01_score': f"{sample_paper['similarities'][0]['similarity']:.6f}"
            }

            # HBase REST API í˜¸ì¶œ
            cells = []
            for column, value in test_data.items():
                cells.append({
                    "column": base64.b64encode(column.encode('utf-8')).decode('utf-8'),
                    "$": base64.b64encode(str(value).encode('utf-8')).decode('utf-8')
                })

            row_data = {
                "Row": [{
                    "key": base64.b64encode("benchmark_test".encode('utf-8')).decode('utf-8'),
                    "Cell": cells
                }]
            }

            response = requests.put(
                f"{hbase_url}/paper_similarities/benchmark_test",
                json=row_data,
                headers={'Content-Type': 'application/json', 'Accept': 'application/json'}
            )

            hbase_time = time.time() - hbase_start

            if response.status_code in [200, 201]:
                print(f"   âœ… HBase ì €ì¥ í…ŒìŠ¤íŠ¸: {hbase_time:.3f}ì´ˆ")

                # ì €ì¥ëœ ë°ì´í„° ì •ë¦¬
                requests.delete(f"{hbase_url}/paper_similarities/benchmark_test")
            else:
                print(f"   âŒ HBase ì €ì¥ ì‹¤íŒ¨: {response.status_code}")

        except Exception as e:
            print(f"   âŒ HBase í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            hbase_time = 0

        # ë…¼ë¬¸ë‹¹ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        total_time = calculation_time + hbase_time
        per_paper_time = total_time / len(embeddings_data)

        self.results['similarity_hbase'].append(per_paper_time)

        print(f"   ğŸ“Š ë…¼ë¬¸ë‹¹ í‰ê·  ì‹œê°„: {per_paper_time:.3f}ì´ˆ")
        print(f"   ğŸ“Š ì²˜ë¦¬ ì†ë„: {3600/per_paper_time:.0f}ë…¼ë¬¸/ì‹œê°„")

        return [per_paper_time]

    def run_full_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ ë…¼ë¬¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print("="*80)

        # 1. OpenAlex ë©”íƒ€ë°ì´í„°
        self.benchmark_openalex_metadata(count=3)

        # 2. PDF + GROBID
        self.benchmark_pdf_grobid(sample_size=2)

        # 3. ì„ë² ë”© ìƒì„±
        self.benchmark_embedding_generation(sample_size=3)

        # 4. ìœ ì‚¬ë„ ê³„ì‚°
        self.benchmark_similarity_calculation(sample_size=10)

        # ê²°ê³¼ ìš”ì•½
        self.print_summary()

    def print_summary(self):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½"""
        print("\n" + "="*80)
        print("ğŸ“Š íŒŒì´í”„ë¼ì¸ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("="*80)

        stages = [
            ('OpenAlex ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘', 'openalex_metadata'),
            ('PDF + GROBID ì²˜ë¦¬', 'pdf_grobid'),
            ('ì„ë² ë”© ìƒì„±', 'embedding'),
            ('ìœ ì‚¬ë„ ê³„ì‚° + HBase', 'similarity_hbase')
        ]

        total_time_per_paper = 0

        for stage_name, key in stages:
            times = self.results[key]
            if times:
                avg_time = statistics.mean(times)
                throughput = 3600 / avg_time
                total_time_per_paper += avg_time

                print(f"\n{stage_name}:")
                print(f"  í‰ê·  ì‹œê°„: {avg_time:.2f}ì´ˆ/ë…¼ë¬¸")
                print(f"  ì²˜ë¦¬ ì†ë„: {throughput:.0f}ë…¼ë¬¸/ì‹œê°„")
                print(f"  ë³‘ëª©ë„: {(avg_time/total_time_per_paper)*100:.1f}%")
            else:
                print(f"\n{stage_name}: ì¸¡ì • ë°ì´í„° ì—†ìŒ")

        print(f"\nğŸ” ì „ì²´ íŒŒì´í”„ë¼ì¸:")
        print(f"  ë…¼ë¬¸ë‹¹ ì´ ì‹œê°„: {total_time_per_paper:.2f}ì´ˆ")
        print(f"  ì „ì²´ ì²˜ë¦¬ ì†ë„: {3600/total_time_per_paper:.0f}ë…¼ë¬¸/ì‹œê°„")
        print(f"  ì¼ì¼ ì²˜ë¦¬ ê°€ëŠ¥ëŸ‰: {24*3600/total_time_per_paper:.0f}ë…¼ë¬¸/ì¼")

        # ë³‘ëª© êµ¬ê°„ ì‹ë³„
        if self.results:
            max_time = 0
            bottleneck = ""
            for stage_name, key in stages:
                if self.results[key]:
                    avg_time = statistics.mean(self.results[key])
                    if avg_time > max_time:
                        max_time = avg_time
                        bottleneck = stage_name

            print(f"\nğŸš¨ ì£¼ìš” ë³‘ëª©: {bottleneck} ({max_time:.2f}ì´ˆ/ë…¼ë¬¸)")

def main():
    benchmark = PipelineBenchmark()
    benchmark.run_full_benchmark()

if __name__ == "__main__":
    main()