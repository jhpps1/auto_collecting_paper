#!/usr/bin/env python3
"""
PySparkë¥¼ ì‚¬ìš©í•œ ëŒ€ê·œëª¨ ë…¼ë¬¸ ìœ ì‚¬ë„ ê³„ì‚°

PostgreSQLì—ì„œ ì„ë² ë”© ë²¡í„°ë¥¼ ì½ì–´ì™€ ëª¨ë“  ë…¼ë¬¸ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ 
Top-K ê²°ê³¼ë¥¼ HBaseì— ì €ì¥
"""

import os
import sys
import json
import numpy as np
from typing import List, Tuple, Dict, Any
import psycopg2
import psycopg2.extras
import requests
from datetime import datetime
import base64

class HBaseRestClient:
    """HBase REST API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, base_url: str):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        })

    def create_table(self, table_name: str, column_families: dict):
        """í…Œì´ë¸” ìƒì„±"""
        try:
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            response = self.session.get(f"{self.base_url}/{table_name}/schema")
            if response.status_code == 200:
                print(f"âœ… í…Œì´ë¸” '{table_name}' ì´ë¯¸ ì¡´ì¬")
                return True

            # í…Œì´ë¸” ìƒì„± ìŠ¤í‚¤ë§ˆ
            schema = {
                "name": table_name,
                "ColumnSchema": [
                    {"name": cf, "COMPRESSION": "NONE"}
                    for cf in column_families.keys()
                ]
            }

            response = self.session.put(
                f"{self.base_url}/{table_name}/schema",
                json=schema
            )

            if response.status_code in [200, 201]:
                print(f"âœ… í…Œì´ë¸” '{table_name}' ìƒì„± ì™„ë£Œ")
                return True
            else:
                print(f"âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return False

        except Exception as e:
            print(f"âŒ í…Œì´ë¸” ìƒì„± ì˜¤ë¥˜: {e}")
            return False

    def put_row(self, table_name: str, row_key: str, data: dict):
        """í–‰ ë°ì´í„° ì €ì¥"""
        try:
            import base64
            # HBase REST API í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
            cells = []
            for column, value in data.items():
                if isinstance(column, bytes):
                    column = column.decode('utf-8')
                if isinstance(value, bytes):
                    value = value.decode('utf-8')

                # ì»¬ëŸ¼ íŒ¨ë°€ë¦¬:ì»¬ëŸ¼ ë¶„ë¦¬
                if ':' in column:
                    cf_col = column
                else:
                    cf_col = f"cf:{column}"

                cells.append({
                    "column": base64.b64encode(cf_col.encode('utf-8')).decode('utf-8'),
                    "$": base64.b64encode(str(value).encode('utf-8')).decode('utf-8')
                })

            row_data = {
                "Row": [{
                    "key": base64.b64encode(row_key.encode('utf-8')).decode('utf-8'),
                    "Cell": cells
                }]
            }

            response = self.session.put(
                f"{self.base_url}/{table_name}/{row_key}",
                json=row_data
            )

            if response.status_code not in [200, 201]:
                print(f"âŒ HBase PUT ì‹¤íŒ¨: {response.status_code} - {response.text}")
                return False

            return True

        except Exception as e:
            print(f"âŒ í–‰ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

    def get_row(self, table_name: str, row_key: str):
        """í–‰ ë°ì´í„° ì¡°íšŒ"""
        try:
            response = self.session.get(f"{self.base_url}/{table_name}/{row_key}")
            if response.status_code == 200:
                return response.json()
            return None
        except Exception as e:
            print(f"âŒ í–‰ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None

# PySpark ì„¤ì •
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.ml.feature import BucketedRandomProjectionLSH
from pyspark.sql import Row

# PostgreSQL ì—°ê²° ì„¤ì •
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'papers_db',
    'user': 'postgres',
    'password': 'postgres123'
}

# HBase REST API ì—°ê²° ì„¤ì •
HBASE_CONFIG = {
    'host': 'localhost',
    'port': 8080,
    'rest_url': 'http://localhost:8080'
}

class PySparkSimilarityCalculator:
    def __init__(self, top_k: int = 30):
        """
        PySpark ìœ ì‚¬ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™”

        Args:
            top_k: ê° ë…¼ë¬¸ë‹¹ ìœ ì‚¬í•œ ë…¼ë¬¸ ìƒìœ„ Kê°œ
        """
        self.top_k = top_k
        self.spark = None
        self.setup_spark()

    def setup_spark(self):
        """Spark ì„¸ì…˜ ì„¤ì •"""
        try:
            print("ğŸ”§ Spark ì„¸ì…˜ ì„¤ì • ì¤‘...")

            # Spark ì„¤ì •
            self.spark = SparkSession.builder \
                .appName("PaperSimilarityCalculation") \
                .config("spark.executor.memory", "2g") \
                .config("spark.driver.memory", "2g") \
                .config("spark.executor.cores", "2") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .getOrCreate()

            # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
            self.spark.sparkContext.setLogLevel("WARN")

            print(f"âœ… Spark ì„¸ì…˜ ì‹œì‘ ì™„ë£Œ")
            print(f"   Spark Version: {self.spark.version}")
            print(f"   Total Cores: {self.spark.sparkContext.defaultParallelism}")

        except Exception as e:
            print(f"âŒ Spark ì„¸ì…˜ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    def load_embeddings_from_postgres(self) -> List[Dict[str, Any]]:
        """
        PostgreSQLì—ì„œ ì„ë² ë”© ë°ì´í„° ë¡œë“œ

        Returns:
            ë…¼ë¬¸ IDì™€ ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            print("ğŸ“¥ PostgreSQLì—ì„œ ì„ë² ë”© ë°ì´í„° ë¡œë”©...")

            with psycopg2.connect(**DB_CONFIG) as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:

                    cursor.execute("""
                        SELECT
                            id,
                            title,
                            embedding,
                            embedding_model,
                            embedding_generated_at
                        FROM papers
                        WHERE embedding IS NOT NULL
                        ORDER BY id
                    """)

                    papers = cursor.fetchall()

            print(f"âœ… {len(papers)}ê°œ ë…¼ë¬¸ì˜ ì„ë² ë”© ë¡œë”© ì™„ë£Œ")

            # ì„ë² ë”© ë²¡í„° íŒŒì‹±
            embeddings_data = []
            for paper in papers:
                try:
                    # pgvector í˜•ì‹ì˜ ë²¡í„°ë¥¼ numpy arrayë¡œ ë³€í™˜
                    embedding_str = paper['embedding']
                    if embedding_str.startswith('[') and embedding_str.endswith(']'):
                        embedding_str = embedding_str[1:-1]

                    embedding_values = [float(x.strip()) for x in embedding_str.split(',')]
                    embedding_array = np.array(embedding_values, dtype=np.float32)

                    embeddings_data.append({
                        'paper_id': paper['id'],
                        'title': paper['title'],
                        'embedding': embedding_array,
                        'embedding_model': paper['embedding_model']
                    })

                except Exception as e:
                    print(f"âš ï¸ ë…¼ë¬¸ ID {paper['id']} ì„ë² ë”© íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

            print(f"ğŸ“Š íŒŒì‹± ì™„ë£Œ: {len(embeddings_data)}ê°œ ë²¡í„°")
            return embeddings_data

        except Exception as e:
            print(f"âŒ ì„ë² ë”© ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return []

    def create_spark_dataframe(self, embeddings_data: List[Dict[str, Any]]):
        """
        ì„ë² ë”© ë°ì´í„°ë¡œ Spark DataFrame ìƒì„±

        Args:
            embeddings_data: ì„ë² ë”© ë°ì´í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            Spark DataFrame
        """
        try:
            print("ğŸ”„ Spark DataFrame ìƒì„± ì¤‘...")

            # Row ê°ì²´ë¡œ ë³€í™˜
            rows = []
            for item in embeddings_data:
                # ì„ë² ë”© ë²¡í„°ë¥¼ Spark ML Vectorë¡œ ë³€í™˜
                vector = Vectors.dense(item['embedding'].tolist())
                rows.append(Row(
                    paper_id=item['paper_id'],
                    title=item['title'],
                    embedding=vector,
                    embedding_model=item['embedding_model']
                ))

            # DataFrame ìƒì„±
            schema = StructType([
                StructField("paper_id", IntegerType(), False),
                StructField("title", StringType(), True),
                StructField("embedding", VectorUDT(), False),
                StructField("embedding_model", StringType(), True)
            ])

            df = self.spark.createDataFrame(rows, schema)
            df.cache()  # ë©”ëª¨ë¦¬ ìºì‹±

            print(f"âœ… DataFrame ìƒì„± ì™„ë£Œ: {df.count()}ê°œ í–‰")
            return df

        except Exception as e:
            print(f"âŒ DataFrame ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def calculate_pairwise_similarity(self, df):
        """
        ëª¨ë“  ë…¼ë¬¸ ìŒì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°

        Args:
            df: ì„ë² ë”©ì´ í¬í•¨ëœ Spark DataFrame

        Returns:
            ìœ ì‚¬ë„ ê²°ê³¼ DataFrame
        """
        try:
            print("ğŸ”¢ ë…¼ë¬¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")

            # ìê¸° ìì‹ ê³¼ì˜ ì¡°ì¸ì„ ìœ„í•´ ë³„ì¹­ ì‚¬ìš©
            df_a = df.alias("a")
            df_b = df.alias("b")

            # ì¹´í‹°ì‹œì•ˆ ê³±ìœ¼ë¡œ ëª¨ë“  ìŒ ìƒì„± (ìê¸° ìì‹  ì œì™¸)
            pairs_df = df_a.crossJoin(df_b).where(col("a.paper_id") < col("b.paper_id"))

            print(f"ğŸ“Š ê³„ì‚°í•  ë…¼ë¬¸ ìŒ ìˆ˜: {pairs_df.count()}ê°œ")

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° UDF ì •ì˜
            def cosine_similarity(v1, v2):
                """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
                try:
                    # Spark Vectorë¥¼ numpy arrayë¡œ ë³€í™˜
                    arr1 = np.array(v1.toArray())
                    arr2 = np.array(v2.toArray())

                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    dot_product = np.dot(arr1, arr2)
                    norm_a = np.linalg.norm(arr1)
                    norm_b = np.linalg.norm(arr2)

                    if norm_a == 0 or norm_b == 0:
                        return 0.0

                    similarity = dot_product / (norm_a * norm_b)
                    return float(similarity)

                except Exception as e:
                    print(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
                    return 0.0

            # UDF ë“±ë¡
            cosine_similarity_udf = udf(cosine_similarity, DoubleType())

            # ìœ ì‚¬ë„ ê³„ì‚°
            similarity_df = pairs_df.withColumn(
                "similarity",
                cosine_similarity_udf(col("a.embedding"), col("b.embedding"))
            ).select(
                col("a.paper_id").alias("paper_id_1"),
                col("a.title").alias("title_1"),
                col("b.paper_id").alias("paper_id_2"),
                col("b.title").alias("title_2"),
                col("similarity")
            )

            # ìœ ì‚¬ë„ê°€ 0ë³´ë‹¤ í° ê²ƒë§Œ í•„í„°ë§
            similarity_df = similarity_df.filter(col("similarity") > 0)

            print(f"âœ… ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ")
            return similarity_df

        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None

    def get_top_k_similarities_per_paper(self, similarity_df):
        """
        ê° ë…¼ë¬¸ë‹¹ ìƒìœ„ Kê°œ ìœ ì‚¬ ë…¼ë¬¸ ì¶”ì¶œ

        Args:
            similarity_df: ìœ ì‚¬ë„ DataFrame

        Returns:
            Top-K ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            print(f"ğŸ” ê° ë…¼ë¬¸ë‹¹ ìƒìœ„ {self.top_k}ê°œ ìœ ì‚¬ ë…¼ë¬¸ ì¶”ì¶œ...")

            # ëŒ€ì¹­ì„±ì„ ìœ„í•´ ì–‘ë°©í–¥ ê´€ê³„ ìƒì„±
            df1 = similarity_df.select(
                col("paper_id_1").alias("source_paper"),
                col("title_1").alias("source_title"),
                col("paper_id_2").alias("target_paper"),
                col("title_2").alias("target_title"),
                col("similarity")
            )

            df2 = similarity_df.select(
                col("paper_id_2").alias("source_paper"),
                col("title_2").alias("source_title"),
                col("paper_id_1").alias("target_paper"),
                col("title_1").alias("target_title"),
                col("similarity")
            )

            # ë‘ ë°©í–¥ ê²°í•©
            bidirectional_df = df1.union(df2)

            # ê° ë…¼ë¬¸ì— ëŒ€í•´ ìƒìœ„ Kê°œ ìœ ì‚¬ ë…¼ë¬¸ ì„ íƒ
            from pyspark.sql.window import Window

            window_spec = Window.partitionBy("source_paper").orderBy(desc("similarity"))
            top_k_df = bidirectional_df.withColumn(
                "rank",
                row_number().over(window_spec)
            ).filter(col("rank") <= self.top_k)

            # ê²°ê³¼ ìˆ˜ì§‘
            results = top_k_df.collect()

            print(f"âœ… Top-K ì¶”ì¶œ ì™„ë£Œ: {len(results)}ê°œ ê´€ê³„")

            # ë…¼ë¬¸ë³„ë¡œ ê·¸ë£¹í™”
            paper_similarities = {}
            for row in results:
                source_id = row['source_paper']
                if source_id not in paper_similarities:
                    paper_similarities[source_id] = {
                        'source_title': row['source_title'],
                        'similarities': []
                    }

                paper_similarities[source_id]['similarities'].append({
                    'target_paper_id': row['target_paper'],
                    'target_title': row['target_title'],
                    'similarity': row['similarity'],
                    'rank': row['rank']
                })

            return paper_similarities

        except Exception as e:
            print(f"âŒ Top-K ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def setup_hbase_table(self):
        """HBase í…Œì´ë¸” ì„¤ì •"""
        try:
            print("ğŸ”§ HBase REST API ì—°ê²° ë° í…Œì´ë¸” ì„¤ì •...")

            hbase_client = HBaseRestClient(HBASE_CONFIG['rest_url'])
            table_name = 'paper_similarities'

            # í…Œì´ë¸” ìƒì„± (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
            column_families = {
                'similar': {},  # ìœ ì‚¬ë„ ì •ë³´
                'meta': {}     # ë©”íƒ€ë°ì´í„°
            }

            success = hbase_client.create_table(table_name, column_families)
            if success:
                return hbase_client, table_name
            else:
                return None, None

        except Exception as e:
            print(f"âŒ HBase ì„¤ì • ì‹¤íŒ¨: {e}")
            return None, None

    def save_similarities_to_hbase(self, paper_similarities: Dict[int, Dict], hbase_client_table):
        """
        ìœ ì‚¬ë„ ê²°ê³¼ë¥¼ HBaseì— ì €ì¥

        Args:
            paper_similarities: ë…¼ë¬¸ë³„ ìœ ì‚¬ë„ ê²°ê³¼
            hbase_client_table: (HBase í´ë¼ì´ì–¸íŠ¸, í…Œì´ë¸”ëª…) íŠœí”Œ
        """
        try:
            print("ğŸ’¾ HBaseì— ìœ ì‚¬ë„ ê²°ê³¼ ì €ì¥ ì¤‘...")

            hbase_client, table_name = hbase_client_table
            saved_count = 0

            for source_paper_id, data in paper_similarities.items():
                try:
                    # Row Key ìƒì„± (paper_id ê¸°ë°˜)
                    row_key = f"paper_{source_paper_id:08d}"

                    # HBaseì— ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
                    hbase_data = {}

                    # ë©”íƒ€ë°ì´í„°
                    hbase_data['meta:source_title'] = data['source_title']
                    hbase_data['meta:total_similarities'] = str(len(data['similarities']))
                    hbase_data['meta:calculated_at'] = datetime.now().isoformat()
                    hbase_data['meta:model'] = 'all-mpnet-base-v2'

                    # ìœ ì‚¬ë„ ì •ë³´ (Top-K)
                    for sim in data['similarities']:
                        rank = sim['rank']
                        prefix = f"similar:rank_{rank:02d}"

                        hbase_data[f"{prefix}_paper_id"] = str(sim['target_paper_id'])
                        hbase_data[f"{prefix}_title"] = sim['target_title']
                        hbase_data[f"{prefix}_score"] = f"{sim['similarity']:.6f}"

                    # ì „ì²´ ìœ ì‚¬ë„ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œë„ ì €ì¥
                    similarities_json = json.dumps({
                        'source_paper_id': source_paper_id,
                        'similarities': data['similarities']
                    }, ensure_ascii=False)
                    hbase_data['similar:full_data'] = similarities_json

                    # HBaseì— ì €ì¥
                    success = hbase_client.put_row(table_name, row_key, hbase_data)
                    if success:
                        saved_count += 1

                    if saved_count % 10 == 0:
                        print(f"  ğŸ’¾ {saved_count}ê°œ ë…¼ë¬¸ ì €ì¥ ì™„ë£Œ...")

                except Exception as e:
                    print(f"âš ï¸ ë…¼ë¬¸ ID {source_paper_id} ì €ì¥ ì‹¤íŒ¨: {e}")
                    continue

            print(f"âœ… HBase ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ë…¼ë¬¸")

        except Exception as e:
            print(f"âŒ HBase ì €ì¥ ì‹¤íŒ¨: {e}")

    def verify_hbase_data(self, hbase_client_table, sample_paper_id: int = None):
        """
        HBaseì— ì €ì¥ëœ ë°ì´í„° ê²€ì¦

        Args:
            hbase_client_table: (HBase í´ë¼ì´ì–¸íŠ¸, í…Œì´ë¸”ëª…) íŠœí”Œ
            sample_paper_id: ê²€ì¦í•  ìƒ˜í”Œ ë…¼ë¬¸ ID
        """
        try:
            print("ğŸ” HBase ë°ì´í„° ê²€ì¦ ì¤‘...")

            hbase_client, table_name = hbase_client_table

            # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
            if sample_paper_id:
                row_key = f"paper_{sample_paper_id:08d}"
                row_data = hbase_client.get_row(table_name, row_key)

                if row_data:
                    print(f"ğŸ“‹ ìƒ˜í”Œ ë…¼ë¬¸ ID {sample_paper_id} ë°ì´í„°:")
                    print(f"  âœ… ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë¨")
                    # JSON ì‘ë‹µì—ì„œ ì‹¤ì œ ë°ì´í„° íŒŒì‹±ì€ ë³µì¡í•˜ë¯€ë¡œ ê°„ë‹¨íˆ í™•ì¸ë§Œ
                else:
                    print(f"âš ï¸ ìƒ˜í”Œ ë…¼ë¬¸ ID {sample_paper_id} ë°ì´í„° ì—†ìŒ")
            else:
                print("ğŸ“Š HBase ì—°ê²° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ HBase ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")

    def run_similarity_calculation(self):
        """ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚° í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            print("ğŸš€ ë…¼ë¬¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...")

            # 1. PostgreSQLì—ì„œ ì„ë² ë”© ë¡œë“œ
            embeddings_data = self.load_embeddings_from_postgres()
            if not embeddings_data:
                print("âŒ ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return

            # 2. Spark DataFrame ìƒì„±
            df = self.create_spark_dataframe(embeddings_data)
            if df is None:
                return

            # 3. ìœ ì‚¬ë„ ê³„ì‚°
            similarity_df = self.calculate_pairwise_similarity(df)
            if similarity_df is None:
                return

            # 4. Top-K ì¶”ì¶œ
            paper_similarities = self.get_top_k_similarities_per_paper(similarity_df)
            if not paper_similarities:
                return

            # 5. HBase í…Œì´ë¸” ì„¤ì •
            hbase_client_table = self.setup_hbase_table()
            if hbase_client_table[0] is None:
                return

            # 6. HBaseì— ì €ì¥
            self.save_similarities_to_hbase(paper_similarities, hbase_client_table)

            # 7. ê²€ì¦
            sample_paper_id = list(paper_similarities.keys())[0] if paper_similarities else None
            self.verify_hbase_data(hbase_client_table, sample_paper_id)

            print("ğŸ‰ ë…¼ë¬¸ ìœ ì‚¬ë„ ê³„ì‚° ë° ì €ì¥ ì™„ë£Œ!")

        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")

        finally:
            if self.spark:
                self.spark.stop()
                print("ğŸ›‘ Spark ì„¸ì…˜ ì¢…ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # Top-30 ìœ ì‚¬ ë…¼ë¬¸ ê³„ì‚°
    calculator = PySparkSimilarityCalculator(top_k=30)
    calculator.run_similarity_calculation()

if __name__ == "__main__":
    main()